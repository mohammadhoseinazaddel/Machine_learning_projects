{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "write  scarp python file that crawl this address https://www.theguardian.com/theguardian/series/the-quiz-thomas-eaton question and answers in each url \n",
        "\n",
        "- all things should run within python running file without runnig linux command\n",
        "- contents should save in csv file "
      ],
      "metadata": {
        "id": "OPWpWSXHJTSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scrapy"
      ],
      "metadata": {
        "id": "20GqQ-wLKuHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i2vzhEYDORN"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import scrapy\n",
        "\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# The quiz is split into regular questions and \"what links\" questions. Regular\n",
        "# questions come first.\n",
        "NUM_REGULAR_QUESTIONS = 8\n",
        "TOTAL_QUESTIONS = 15\n",
        "\n",
        "\n",
        "class QuizScraper(scrapy.Spider):\n",
        "    \"\"\"A scraper for fetching the Guardian weekend quiz\"\"\"\n",
        "    name = 'quiz-scraper'\n",
        "    start_urls = [\n",
        "        'https://www.theguardian.com/theguardian/series/the-quiz-thomas-eaton'\n",
        "    ]\n",
        "    # custom_settings={'FEED_URI':\"data_%(time).json\",\n",
        "    #                  'FEED_FORMAT': \"json\"}\n",
        "    custom_settings={'FEEDS': {'csv_file.csv': {'format': 'csv'}}}\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Parses the start URL, which is the index page for the quiz. It finds\n",
        "        # the first quiz link (i.e. the link to the latest quiz) and passes it\n",
        "        # to parse_quiz\n",
        "        quiz_containers = response.css('section.fc-container')\n",
        "        latest_quiz_url = quiz_containers[0].css('div.fc-item__container a ::attr(href)').extract_first()\n",
        "        print(\"******************************************\")\n",
        "        print(latest_quiz_url)\n",
        "        print(\"******************************************\")\n",
        "        if latest_quiz_url:\n",
        "            yield scrapy.Request(latest_quiz_url, callback=self.parse_quiz)\n",
        "\n",
        "    def parse_quiz(self, response):\n",
        "        # Takes the content of a single quiz page and parses it\n",
        "        quiz_content = response.css('div.article-body-commercial-selector p::text').extract()\n",
        "        quiz_content = [q.strip() for q in quiz_content if len(q.strip()) > 0]\n",
        "        print(\"******************************************\")\n",
        "        print(\"******************************************\")\n",
        "        print(quiz_content)\n",
        "        print(\"******************************************\")\n",
        "        print(\"******************************************\")\n",
        "        total_elements = TOTAL_QUESTIONS * 2\n",
        "        yield {'quiz_content':quiz_content}\n",
        "\n",
        "def main():\n",
        "    process = CrawlerProcess({})\n",
        "\n",
        "    process.crawl(QuizScraper)\n",
        "    process.start()\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### review\n"
      ],
      "metadata": {
        "id": "5h2BXTmGLnWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "write linux command to run postgress docker with volume username password as environments and runs on port 5432\n",
        "\n",
        "after running it show containers and images that are running on your machine and past the screenshot pf them here"
      ],
      "metadata": {
        "id": "z3S3qRThKz8Z"
      }
    }
  ]
}