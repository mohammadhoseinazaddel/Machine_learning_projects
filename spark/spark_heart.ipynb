{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d687ae71",
      "metadata": {},
      "source": [
        "https://docs.databricks.com/_static/notebooks/xgboost-pyspark.html\n",
        "\n",
        "https://www.databricks.com/blog/2020/11/16/how-to-train-xgboost-with-spark.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed69262",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "#Create Session\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265c7cd6",
      "metadata": {},
      "source": [
        "run locally\n",
        "\n",
        "\n",
        "https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext\n",
        "\n",
        "https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkContext.html#:~:text=A%20SparkContext%20represents%20the%20connection,parameters%20here%20or%20through%20conf%20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf27e5b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import findspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkConf, SparkContextfindspark.init('/spark-3.3.0-bin-hadoop3.2') \n",
        "#The key here is putting the path to the spark download on your Mac\n",
        "VM.sc=pyspark.SparkContext(master='spark://ip:port',appName='Heart_Disease_Example')\n",
        "#Use the same 'spark://ip:port' from connecting the worker(s) to the master node. \n",
        "spark = SQLContext(sc)\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243423f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import numpy as np\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "import plotly.graph_objects as go\n",
        "df=spark.read.csv('heart.csv', inferSchema=True, header=True)\n",
        "df.count()\n",
        "len(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e651210",
      "metadata": {},
      "source": [
        " has 303 rows and 14 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a8ea8a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64b697b1",
      "metadata": {},
      "source": [
        "1. age: The person’s age in years\n",
        "\n",
        "2. sex: The person’s sex (1 = male, 0 = female)\n",
        "\n",
        "3. cp: The chest pain experienced (0 = typical angina, 1= atypical angina, 2= non-anginal pain, 3 = asymptomatic)\n",
        "\n",
        "4. trestbps: The person’s resting blood pressure (mm Hg on admission to the hospital)\n",
        "\n",
        "5. chol: The person’s cholesterol measurement in mg/dl\n",
        "\n",
        "6. fbs: The person’s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false).\n",
        "\n",
        "7. restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes’ criteria)\n",
        "\n",
        "8. thalach: The person’s maximum heart rate achieved\n",
        "\n",
        "9. exang: Exercise induced angina (1 = yes; 0 = no)\n",
        "\n",
        "10. oldpeak: ST depression induced by exercise relative to rest\n",
        "\n",
        "11. slope: the slope of the peak exercise ST segment (0 = upsloping, 1 = flat, 2 = downsloping)\n",
        "\n",
        "12. ca: The number of major vessels (0–4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3e17dab",
      "metadata": {},
      "source": [
        "Check for missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b06e35a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col,sum\n",
        "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09420e75",
      "metadata": {},
      "source": [
        "Summary of Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffba0e1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe().toPandas().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81a02850",
      "metadata": {},
      "source": [
        "Pie Chart of Target Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70239f19",
      "metadata": {},
      "outputs": [],
      "source": [
        "df2=df.toPandas()\n",
        "df22=df2.groupby('target').count().reset_index()[['target','age']].rename(columns={'age':'counts'})\n",
        "colors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen']\n",
        "fig = go.Figure(data=[go.Pie(labels=df22.target,\n",
        "                             values=df22.counts)])\n",
        "fig.update_traces(hoverinfo='label+percent', textinfo='value+percent', textfont_size=20, textfont_color='black',\n",
        "                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n",
        "# fig.show()\n",
        "fig.update_layout(title='Heart Disease vs. Absence of Heart Disease', title_x=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4568adbf",
      "metadata": {},
      "source": [
        "Histograms of Feature Variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c8a464",
      "metadata": {},
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "fig = make_subplots(rows=4, cols=4, start_cell=\"top-left\",\n",
        "                   subplot_titles=df2.columns[:-1])\n",
        "fig.add_trace(go.Histogram(x=df2.age, name='age'),\n",
        "              row=1, col=1)\n",
        "fig.add_trace(go.Histogram(x=df2.sex, name='sex'),\n",
        "              row=1, col=2)\n",
        "fig.add_trace(go.Histogram(x=df2.cp, name='cp'),\n",
        "              row=1, col=3)\n",
        "fig.add_trace(go.Histogram(x=df2.trestbps, name='trestbps'),\n",
        "              row=1, col=4)\n",
        "fig.add_trace(go.Histogram(x=df2.chol, name='chol'),\n",
        "              row=2, col=1)\n",
        "fig.add_trace(go.Histogram(x=df2.fbs, name='fbs'),\n",
        "              row=2, col=2)\n",
        "fig.add_trace(go.Histogram(x=df2.restecg, name='restecg'),\n",
        "              row=2, col=3)\n",
        "fig.add_trace(go.Histogram(x=df2.thalach, name='thalach'),\n",
        "              row=2, col=4)\n",
        "fig.add_trace(go.Histogram(x=df2.exang, name='exang'),\n",
        "              row=3, col=1)\n",
        "fig.add_trace(go.Histogram(x=df2.oldpeak, name='oldpeak'),\n",
        "              row=3, col=2)\n",
        "fig.add_trace(go.Histogram(x=df2.slope, name='slope'),\n",
        "              row=3, col=3)\n",
        "fig.add_trace(go.Histogram(x=df2.thalach, name='ca'),\n",
        "              row=3, col=4)\n",
        "fig.add_trace(go.Histogram(x=df2.thal, name='thal'),\n",
        "              row=4, col=1)\n",
        "fig.update_layout(title='Histograms of Variables', title_x=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732e6a31",
      "metadata": {},
      "source": [
        "Correlation Matrix Heatmap:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d379b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "df3=df.withColumn('oldpeaklog', F.log(df['oldpeak']+1))\n",
        "df33=df3.toPandas()\n",
        "fig = make_subplots(rows=1, cols=2, start_cell=\"top-left\",\n",
        "                   subplot_titles=['oldpeak','oldpeaklog'])\n",
        "fig.add_trace(go.Histogram(x=df33.oldpeak, name='oldpeak'),\n",
        "              row=1, col=1)\n",
        "fig.add_trace(go.Histogram(x=df33.oldpeaklog, name='oldpeaklog'),\n",
        "              row=1, col=2)\n",
        "fig.update_layout(title='Transforming oldpeak', title_x=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3208d59c",
      "metadata": {},
      "outputs": [],
      "source": [
        "corr = df33.corr()\n",
        "fig = go.Figure(data=go.Heatmap(z=corr.values,\n",
        " x=corr.index.values,\n",
        " y=corr.columns.values,\n",
        " text=np.round(corr.values,2),\n",
        " texttemplate=f\"{text}\"))\n",
        "fig.update_layout(title=dict(text='Correlation Matrix Heatmap',font=dict(size=20), x=0.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7a3d54a",
      "metadata": {},
      "source": [
        "Initialize Stages\n",
        "\n",
        "StringIndexer:\n",
        "\n",
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html\n",
        "\n",
        "VectorAssembler:\n",
        "\n",
        "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.feature.VectorAssembler.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4129325",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Initialize stages\n",
        "stages = []\n",
        "#Target column\n",
        "label_stringIdx = StringIndexer(inputCol = 'target', outputCol = 'label')\n",
        "stages += [label_stringIdx]\n",
        "#Numeric Columns\n",
        "numericCols = ['age',\n",
        " 'sex',\n",
        " 'cp',\n",
        " 'trestbps',\n",
        " 'chol',\n",
        " 'fbs',\n",
        " 'restecg',\n",
        " 'thalach',\n",
        " 'exang',\n",
        " 'slope',\n",
        " 'ca',\n",
        " 'thal',\n",
        " 'oldpeaklog'] \n",
        "#Create a vector assembler\n",
        "assemblerInputs = numericCols \n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\").setHandleInvalid('keep')\n",
        "stages += [assembler]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649cdefd",
      "metadata": {},
      "source": [
        "Set up Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa7cc43",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages = stages)\n",
        "pipelineModel = pipeline.fit(df3)\n",
        "df3 = pipelineModel.transform(df3)\n",
        "selectedCols = ['label', 'features'] + ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','slope','ca','thal','oldpeaklog','target']\n",
        "df3 = df3.select(selectedCols)\n",
        "df3.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f1ea8da",
      "metadata": {},
      "source": [
        "Split into training and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0242bda",
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = df3.randomSplit([0.7, 0.3], seed = 2018)\n",
        "train.groupby('target').count().show()\n",
        "test.groupby('target').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59098142",
      "metadata": {},
      "source": [
        "## Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ef9752c",
      "metadata": {},
      "source": [
        "### Random Forests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af7a498",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label', seed=101)\n",
        "rfModel = rf.fit(train)\n",
        "predictions_rf=rfModel.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "626d9141",
      "metadata": {},
      "source": [
        "Accuracy of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed542a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "evaluator_rf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_rf.evaluate(predictions_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12861eff",
      "metadata": {},
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f6ef11",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_rf.crosstab('label','prediction').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da70d199",
      "metadata": {},
      "source": [
        "ROC & Precision Recall Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fa0cfc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from handyspark import *\n",
        "# Creates instance of extended version of BinaryClassificationMetrics\n",
        "# using a DataFrame and its probability and label columns, as the output\n",
        "# from the classifier\n",
        "bcm = BinaryClassificationMetrics(predictions_rf, scoreCol='probability', labelCol='label')\n",
        "# Get metrics from evaluator\n",
        "print(\"Area under ROC Curve: {:.4f}\".format(bcm.areaUnderROC))\n",
        "print(\"Area under PR Curve: {:.4f}\".format(bcm.areaUnderPR))\n",
        "# Plot both ROC and PR curves\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "bcm.plot_roc_curve(ax=axs[0])\n",
        "bcm.plot_pr_curve(ax=axs[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "029a65df",
      "metadata": {},
      "source": [
        "Testing Various Thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b46058",
      "metadata": {},
      "outputs": [],
      "source": [
        "split1_udf = F.udf(lambda value: value[0].item(), FloatType())\n",
        "split2_udf = F.udf(lambda value: value[1].item(), FloatType())\n",
        "def test_threshold(model, prob):\n",
        "    output2 = model.select('rawPrediction','target','probability',split1_udf('probability').alias('class_0'), split2_udf('probability').alias('class_1'))\n",
        "    from pyspark.sql.functions import col, when\n",
        "    output2=output2.withColumn('prediction', when(col('class_0')> prob, 1).otherwise(0))\n",
        "    output2.crosstab('prediction','target').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fb6030",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_threshold(predictions_rf,.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b46d6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_threshold(predictions_rf,.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d0b95c",
      "metadata": {},
      "source": [
        "Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cea343c",
      "metadata": {},
      "outputs": [],
      "source": [
        "feat_imps=rfModel.featureImportances\n",
        "x_values = list(range(len(feat_imps)))\n",
        "plt.bar(x_values, feat_imps, orientation = 'vertical')\n",
        "plt.xticks(x_values, ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','slope','ca','thal','oldpeaklog'], rotation=40)\n",
        "plt.ylabel('Importance')\n",
        "plt.xlabel('Feature')\n",
        "plt.title('Feature Importances')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869ffc2d",
      "metadata": {},
      "source": [
        "cp, thalach, ca, and oldpeaklog\n",
        "#### Tune Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9cc3e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "paramGrid_rf = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [int(x) for x in np.arange(200,221,10)]) \\\n",
        "    .addGrid(rf.maxDepth, [int(x) for x in np.arange(10,11,10)]) \\\n",
        "    .addGrid(rf.featureSubsetStrategy, [x for x in [\"sqrt\", \"log2\", \"onethird\"]]) \\\n",
        "    .addGrid(rf.impurity, [x for x in ['gini','entropy']]) \\\n",
        "    .addGrid(rf.maxBins, [int(x) for x in np.arange(22, 42, 10)]) \\\n",
        "    .build()\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "rf_crossval = CrossValidator(estimator=rf,\n",
        "                          estimatorParamMaps=paramGrid_rf,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "rf_cvModel = rf_crossval.fit(train)\n",
        "predictions_rf_cv = rf_cvModel.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e800ea",
      "metadata": {},
      "source": [
        "Accuracy of Best CV Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f3da80",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator_rf_cv = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_rf_cv.evaluate(predictions_rf_cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "790d9435",
      "metadata": {},
      "source": [
        "Feature Importances of Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e48063",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "feat_imps=rf_cvModel.bestModel.featureImportances\n",
        "x_values = list(range(len(feat_imps)))\n",
        "plt.bar(x_values, feat_imps, orientation = 'vertical')\n",
        "plt.xticks(x_values, ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','slope','ca','thal','oldpeaklog'], rotation=40)\n",
        "plt.ylabel('Importance')\n",
        "plt.xlabel('Feature')\n",
        "plt.title('Feature Importances')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1331409",
      "metadata": {},
      "source": [
        "Get Hyperparameter Values of Best Random Forests CV Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d2b5f79",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Num Trees: ' + str(rf_cvModel.bestModel.getNumTrees))\n",
        "print('Max Depth: ' + str(rf_cvModel.bestModel.getMaxDepth()))\n",
        "print('Feature Subset Strategy: ' + str(rf_cvModel.bestModel.getFeatureSubsetStrategy()))\n",
        "print('Impurity: ' + str(rf_cvModel.bestModel.getImpurity()))\n",
        "print('Max Bins: ' + str(rf_cvModel.bestModel.getMaxBins()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78206a2b",
      "metadata": {},
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12323e13",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0,featuresCol = 'features', labelCol = 'label')\n",
        "lrModel = lr.fit(train)\n",
        "predictions_lr = lrModel.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ed9eb9",
      "metadata": {},
      "source": [
        "Accuracy of Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304bb530",
      "metadata": {},
      "source": [
        "evaluator_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_lr.evaluate(predictions_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9f9e94",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_lr.evaluate(predictions_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c36832a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the coefficients and intercept for logistic regression\n",
        "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
        "print(\"Intercept: \" + str(lrModel.intercept))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "839bc493",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_lr.crosstab('label','prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "674eb263",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creates instance of extended version of BinaryClassificationMetrics\n",
        "# using a DataFrame and its probability and label columns, as the output\n",
        "# from the classifier\n",
        "bcm = BinaryClassificationMetrics(predictions_lr, scoreCol='probability', labelCol='label')\n",
        "# Get metrics from evaluator\n",
        "print(\"Area under ROC Curve: {:.4f}\".format(bcm.areaUnderROC))\n",
        "print(\"Area under PR Curve: {:.4f}\".format(bcm.areaUnderPR))\n",
        "# Plot both ROC and PR curves\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "bcm.plot_roc_curve(ax=axs[0])\n",
        "bcm.plot_pr_curve(ax=axs[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18fd1db",
      "metadata": {},
      "source": [
        "Tune Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde49701",
      "metadata": {},
      "outputs": [],
      "source": [
        "paramGrid_lr = ParamGridBuilder() \\\n",
        "    .addGrid(lr.maxIter, [int(x) for x in np.arange(10,30,10)]) \\\n",
        "    .addGrid(lr.regParam, [int(x) for x in np.arange(.1,.5,.1)]) \\\n",
        "    .addGrid(lr.elasticNetParam, [int(x) for x in np.arange(0,.2,.1)]) \\\n",
        "    .build()\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "lr_crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGrid_lr,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "lr_cvModel = lr_crossval.fit(train)\n",
        "predictions_lr_cv = lr_cvModel.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad30ae9",
      "metadata": {},
      "source": [
        "Overall Accuracy of Best CV Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "328da820",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator_lr_cv = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_lr_cv.evaluate(predictions_lr_cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db5304b7",
      "metadata": {},
      "source": [
        "Logistic Regression V.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8552f60a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "glr = GeneralizedLinearRegression(family=\"binomial\", link=\"logit\", maxIter=10, \n",
        "regParam=0.0)\n",
        "model = glr.fit(train)\n",
        "summary = model.summary\n",
        "print('Variables:' + str(train.columns[2:-1]))\n",
        "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
        "print(\"T Values: \" + str(summary.tValues))\n",
        "print(\"P Values: \" + str(summary.pValues))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4817efde",
      "metadata": {},
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972f918a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes(featuresCol = 'features', labelCol = 'label')\n",
        "nb_model = nb.fit(train)\n",
        "predictions_nb=nb_model.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64d5027f",
      "metadata": {},
      "source": [
        "Overall Accuracy of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "499029cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator_nb = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_nb.evaluate(predictions_nb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a36dac",
      "metadata": {},
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44667a3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_nb.crosstab('label','prediction').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a576b38f",
      "metadata": {},
      "source": [
        "ROC and PR Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6951ce3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from handyspark import *\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "# Creates instance of extended version of BinaryClassificationMetrics\n",
        "# using a DataFrame and its probability and label columns, as the output\n",
        "# from the classifier\n",
        "bcm = BinaryClassificationMetrics(predictions_nb, scoreCol='probability', labelCol='label')\n",
        "# Get metrics from evaluator\n",
        "print(\"Area under ROC Curve: {:.4f}\".format(bcm.areaUnderROC))\n",
        "print(\"Area under PR Curve: {:.4f}\".format(bcm.areaUnderPR))\n",
        "# Plot both ROC and PR curves\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "bcm.plot_roc_curve(ax=axs[0])\n",
        "bcm.plot_pr_curve(ax=axs[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a93ae32c",
      "metadata": {},
      "source": [
        "Tune Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea75981",
      "metadata": {},
      "outputs": [],
      "source": [
        "paramGrid_nb = ParamGridBuilder() \\\n",
        "    .addGrid(nb.smoothing, [int(x) for x in np.arange(1,10,1)]) \\\n",
        "    .build()\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "nb_crossval = CrossValidator(estimator=nb,\n",
        "                          estimatorParamMaps=paramGrid_nb,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "nb_cvModel = nb_crossval.fit(train)\n",
        "predictions_nb_cv = nb_cvModel.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99df012f",
      "metadata": {},
      "source": [
        "Evaluate Best CV Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630c5b78",
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator_nb_cv = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator_nb_cv.evaluate(predictions_nb_cv)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Recognizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
